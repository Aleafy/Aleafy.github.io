<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Alpha-CLIP">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Alpha-CLIP: A CLIP Model Focusing on Wherever You Want</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Alpha-CLIP: A CLIP Model Focusing on Wherever You Want</h1>
          <div class="is-size-5 publication-authors"> <span class="author-block"> <a href="https://github.com/SunzeY">Zeyi Sun</a><sup>1,3</sup>,</span> <span class="author-block"> <a href="https://github.com/Aleafy">Ye Fang</a><sup>2,3</sup>,</span> <span class="author-block"> <a href="https://wutong16.github.io/">Tong Wu</a><sup>3,4</sup>, </span> <span class="author-block"> <a href="https://panzhang0212.github.io/">Pan Zhang</a><sup>3</sup>,<br>
          </span> <span class="author-block"> <a href="https://yuhangzang.github.io/">Yuhang Zang</a><sup>3</sup>, </span> <span class="author-block"> <a href="https://aimerykong.github.io/">Shu Kong</a><sup>5,6</sup>, </span> <span class="author-block"> <a href="http://yjxiong.me/">Yuanjun Xiong</a><sup>7</sup>, </span> <span class="author-block"> <a href="http://dahua.site/">Dahua Lin</a><sup>3,4</sup>, </span> <span class="author-block"> <a href="https://myownskyw7.github.io/">Jiaqi Wang</a><sup>3</sup> </span> </div>
          <div class="is-size-5 publication-authors"> <span class="author-block"><sup>1</sup>Shanghai Jiao Tong University,</span> <span class="author-block"><sup>2</sup>Fudan University,</span> <span class="author-block"><sup>3</sup>Shanghai Artificial Intelligence Lab,</span><br>
              <span class="author-block"><sup>4</sup>The Chinese University of Hong Kong,</span> <span class="author-block"><sup>5</sup>Texas A&M University,</span> <span class="author-block"><sup>6</sup>University of Macau,</span> <span class="author-block"><sup>7</sup>Amazon AI</span> </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block"> <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="ai ai-arxiv"></i> </span> <span>arXiv</span> </a> </span>
              <!-- Video Link. -->
              <span class="link-block"> <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="fab fa-youtube"></i> </span> <span>Video</span> </a> </span>
              <!-- Code Link. -->
              <span class="link-block"> <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="fab fa-github"></i> </span> <span>Code</span> </a> </span>
              <!-- Dataset Link. -->
              <span class="link-block"> <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="far fa-images"></i> </span> <span>Dataset</span></a></span></div>
          	 </div>
       	   </div>
        </div>
  </div>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
	  <!--
		<div style="text-align: center;">
				<img id="teaser" width="100%" src="static/images/teaser.png">     
			  </div><br>
	  -->
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Contrastive Language-Image Pre-training (CLIP) plays an essential role in extracting valuable content information from images across diverse tasks. It aligns textual and visual modalities to comprehend the entire image, including all the details, even those irrelevant to specific tasks. However, for a finer understanding and controlled editing of images, it becomes crucial to focus on specific regions of interest, which can be indicated as points, masks, or boxes by humans or perception models.
          </p>
          <p>
            To fulfill the requirements, we introduce <span style="color: orange; font-weight:bold">Alpha-CLIP</span>, an enhanced version of CLIP with an auxiliary alpha channel to suggest attentive regions and fine-tuned with constructed millions of <b><i>RGBA region-text</i></b> pairs. Alpha-CLIP not only preserves the visual recognition ability of CLIP but also enables precise control over the emphasis of image contents. It demonstrates effectiveness in various tasks, including but not limited to <b>open-world recognition</b>, <b>multimodal large language models</b>, and <b>conditional 2D / 3D generation</b>. It has a strong potential to serve as a versatile tool for image-related tasks. All the code, models, and training data will be publicly available.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>

<section class="section"  style="background-color:#efeff081" id="Highlight">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">
            <h2 class="title is-3">üî•Highlight</h2>
            <div class="content has-text-justified">
              <p style="font-size: 15px;">
                <ol>
				  <li><b>Alpha-CLIP Introduction.</b></li>
				  <li><b>Various Downstream Tasks.</b><br>
				  Alpha-CLIP+LLM</li>
				</ol>
              </p>
            </div>
          </div>
        </div>
      </div>
</section>

<section class="section" id="Alpha-CLIP">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"> Alpha-CLIP Pipeline</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>
                We delineate the process of utilizing the seed captions to train a general captioner (Share-Captioner)
                and then employing this captioner to generate <b>1.2M high-quality captions</b> for pre-training usage.
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/pipeline.png">     
                  </div>
              </p>
            </div>
            </b></font>
          </div>
        </div>
      </div>
    </section>


<section class="section" id="Alpha-CLIP">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"> Alpha-CLIP Usage</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>
                We delineate the process of utilizing the seed captions to train a general captioner (Share-Captioner)
                and then employing this captioner to generate <b>1.2M high-quality captions</b> for pre-training usage.
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/usage_table.png">     
                  </div> <br>
				<centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/teaser.png">     
                  </div>
              </p>
            </div>
            </b></font>
          </div>
        </div>
      </div>
    </section>


<section class="section" id="Alpha-CLIP">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"> Alpha-CLIP in MLLM</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>
                We delineate the process of utilizing the seed captions to train a general captioner (Share-Captioner)
                and then employing this captioner to generate <b>1.2M high-quality captions</b> for pre-training usage.
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/llm.png">     
                  </div>
				<!-- 
			 	<centering>
			  	  <div style="text-align: center;">
				    <img id="teaser" width="100%" src="static/images/app_more_llava.png">     
			      </div>
				 -->
              </p>
            </div>
            </b></font>
          </div>
        </div>
      </div>
    </section>


<section class="section" id="Alpha-CLIP">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"> Alpha-CLIP in Image Variation</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>
                We delineate the process of utilizing the seed captions to train a general captioner (Share-Captioner)
                and then employing this captioner to generate <b>1.2M high-quality captions</b> for pre-training usage.
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/append_more_diff.png">     
                  </div>
              </p>
            </div>
            </b></font>
          </div>
        </div>
      </div>
    </section>

<section class="section" id="Alpha-CLIP">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"> Alpha-CLIP in Image Recognition</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>
			  ËøôÈáåÊç¢ÊàêË°®Ê†ºÔºåorÂèØ‰ª•Âä†‰∏Ä‰∏™clsÊµãËØïpipelineÔºõ ‰ª•Âèärec‰ªªÂä° <br>
                We delineate the process of utilizing the seed captions to train a general captioner (Share-Captioner)
                and then employing this captioner to generate <b>1.2M high-quality captions</b> for pre-training usage.
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/pipeline.png">     
                  </div>
              </p>
            </div>
            </b></font>
          </div>
        </div>
      </div>
    </section>

<section class="section" id="Alpha-CLIP">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"> Alpha-CLIP in 3D Object Generation</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>
			  	Âä†ÂÖ•ÈÉ®ÂàÜÂ•ΩÁöÑPoint-e case
				ÂèØ‰ª•ËÄÉËôëÂºÑÊàêÊóãËΩ¨ÁöÑ<br>
                We delineate the process of utilizing the seed captions to train a general captioner (Share-Captioner)
                and then employing this captioner to generate <b>1.2M high-quality captions</b> for pre-training usage.
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/3d_append_pureclip.png">     
                  </div>
              </p>
            </div>
            </b></font>
          </div>
        </div>
      </div>
    </section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
